{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f888e6-c57e-4bb1-9cae-9a87b449d48d",
   "metadata": {},
   "source": [
    "### Web Intelligence - Exercise 11\n",
    "\n",
    "In this exercise, we will explore the core principles and practical implementations of Recurrent Neural Networks (RNNs), one of the fundamental architectures for sequential data modeling. We will begin by building a simple RNN to understand its mechanics, including how hidden states evolve and how sequential dependencies are captured. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e06c9a-3a14-4c49-ad7f-57dac2ca9277",
   "metadata": {},
   "source": [
    "**Question 1.** Classifying Names with a Character-Level RNN\n",
    "\n",
    "We will implement a character-level Recurrent Neural Network (RNN) to classify names by their language of origin in PyTorch. A dataset containing names from different languages can be downloaded from [here](https://download.pytorch.org/tutorial/data.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1158f0-da38-4cf1-98e7-7bef4191a3d3",
   "metadata": {},
   "source": [
    "Import the required packages and set the data folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66393912-c6d5-4b30-baa1-935928397eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:35:54.099957400Z",
     "start_time": "2025-01-11T11:35:41.668703Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the data folder path\n",
    "data_folder = 'data/names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ceb0bd-260b-4ec6-a134-f29f42b28fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:16.848029800Z",
     "start_time": "2024-12-02T14:15:16.834520200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabic.', 'Chinese.', 'Czech.', 'Dutch.', 'English.', 'French.', 'German.', 'Greek.', 'Irish.', 'Italian.', 'Japanese.', 'Korean.', 'Polish.', 'Portuguese.', 'Russian.', 'Scottish.', 'Spanish.', 'Vietnamese.']\n"
     ]
    }
   ],
   "source": [
    "# Print possible languages\n",
    "possible_languages = [filename[:-3] for filename in os.listdir(data_folder)]\n",
    "print(possible_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27db7918-e4ff-460d-8633-a9d1e0875521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:35:54.133963Z",
     "start_time": "2025-01-11T11:35:54.100966600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose languages and read the files.\n",
    "languages = [\"Spanish\", \"French\"]\n",
    "# Set the maximum length of names (discard names whose length larger than the threshold)\n",
    "max_name_length = 15\n",
    "\n",
    "# Read the names stored in the language files.\n",
    "names = []\n",
    "for language in languages:\n",
    "    file_path = os.path.join(data_folder, f\"{language}.txt\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Specify UTF-8 encoding\n",
    "        current_language_names = [line.strip() for line in f if len(line) <= max_name_length]\n",
    "    names.append(current_language_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba99ca-7019-4043-ba62-2ed06081065a",
   "metadata": {},
   "source": [
    "Map letters to character indices and represent names and letters as one-hot encodings.\n",
    "\n",
    "**Note:** Indexing can be started from $1$ and $0$ can be used for unknown characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dca1abe-7c84-48ec-86ff-a7978111f49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:35:59.821724600Z",
     "start_time": "2025-01-11T11:35:59.790184800Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# Map letters to indices. Note that for unknown characters, we can use index 0.\n",
    "letter2idx = {letter: idx+1 for idx, letter in enumerate(string.ascii_letters)}\n",
    "\n",
    "letters_num = len(letter2idx) + 1 # +1 for unknown letters\n",
    "\n",
    "# Map given letters to one-hot encoding\n",
    "def letter2onehot(letter):\n",
    "\n",
    "    one_hot_vect = np.zeros(letters_num, dtype=int)  # Create a zero vector\n",
    "    idx = letter2idx.get(letter, 0)  # Get index for the letter, default to 0 for unknown\n",
    "    one_hot_vect[idx] = 1  # Set the corresponding position to 1\n",
    "    \n",
    "    return one_hot_vect\n",
    "\n",
    "# Convert the \n",
    "def name2onehot(name):\n",
    "    assert len(name) < max_name_length, f\"Given name is longer than {max_name_length}!\"\n",
    "    \n",
    "    # name should be a matrix\n",
    "    \n",
    "    # Convert each letter to one-hot and collect them into a list\n",
    "    # one_hot_enc = [letter2onehot(letter) for letter in name]\n",
    "\n",
    "    one_hot_enc = np.array([letter2onehot(letter) for letter in name])\n",
    "\n",
    "    \n",
    "    # If the name is shorter than max_name_length, pad with zero-vectors\n",
    "    padding_length = max_name_length - len(name)\n",
    "    if padding_length > 0:\n",
    "        padding = np.zeros((padding_length, letters_num), dtype=int)\n",
    "        one_hot_enc = np.vstack([one_hot_enc, padding])  # Combine the one-hot encodings with padding\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    return torch.tensor(one_hot_enc, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344a845-bcbf-4366-8534-c74701370b32",
   "metadata": {},
   "source": [
    "Define the dataset tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec39a38-5b2e-4211-8e9e-c7b6a5c062dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:16.967255500Z",
     "start_time": "2024-12-02T14:15:16.895446500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([575])\n",
      "torch.Size([575, 15, 53])\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.stack(\n",
    "    [name2onehot(name) for language_names in names for name in language_names]\n",
    ")\n",
    "train_labels = torch.as_tensor(\n",
    "    [label for label, language_names in enumerate(names) for _ in language_names],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "# For debugging\n",
    "print(train_labels.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a985339-6ce4-4ca8-8f6e-a644e08408b9",
   "metadata": {},
   "source": [
    "Define an RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb5c538-a9f2-4e81-a0ca-eb940f0c7242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:36:07.134520600Z",
     "start_time": "2025-01-11T11:36:07.118895900Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        print(hidden.shape)\n",
    "        print(input.shape)\n",
    "        out, hidden = self.rnn(input, hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac8b30-76d9-4532-b07c-4a604cca04f3",
   "metadata": {},
   "source": [
    "For training, define the model and optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e689f38-b862-4c91-89f3-2d0f147f91ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:36:24.786485Z",
     "start_time": "2025-01-11T11:36:14.461407100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of languages\n",
    "class_num = len(languages)\n",
    "# Define the model hyper-parameters\n",
    "n_hidden = 2\n",
    "# Define the optimization parameter\n",
    "lr = 0.1\n",
    "epochs_num = 2\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = RNN(letters_num, n_hidden, class_num)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2363a-b913-4b8c-930a-097d256dec13",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4efff2-2906-4fd7-8c09-dfdd75a395a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:36:49.512383500Z",
     "start_time": "2025-01-11T11:36:49.456431600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1af151120784f57835da9469662e7e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs_num)):\n\u001B[0;32m     11\u001B[0m     epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m tqdm(torch\u001B[38;5;241m.\u001B[39mrandperm(\u001B[43mtrain_data\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]), desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrent epoch:\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     14\u001B[0m         input_mat \u001B[38;5;241m=\u001B[39m train_data[idx]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     15\u001B[0m         label \u001B[38;5;241m=\u001B[39m train_labels[idx]\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the platform that we will use for training.\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "device = torch.device('cpu') \n",
    "\n",
    "# Move the model parameters\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for idx in tqdm(torch.randperm(train_data.shape[0]), desc=\"Current epoch:\"):\n",
    "        \n",
    "        input_mat = train_data[idx].to(device)\n",
    "        label = train_labels[idx].to(device).unsqueeze(0)\n",
    "    \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Initialize the hidden state of the model\n",
    "        hidden = model.initHidden().to(device)\n",
    "        # \n",
    "        for letter_enc in input_mat:\n",
    "            output, hidden = model(letter_enc, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-lr)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/train_data.shape[0]:.4f}\")  \n",
    "        \n",
    "# Transfer the model back to the cpu.\n",
    "# model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc4fd6-9177-4fd3-9cf9-cf218e09e387",
   "metadata": {},
   "source": [
    "Implement an evaluation to measure accuracy on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57161337-f198-431f-8c6d-8a3d2e5dacce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:18.629519300Z",
     "start_time": "2024-12-02T14:15:18.624269600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracies = torch.zeros(class_num, class_num)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Complete the implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fa275-4003-4ef8-961c-047929db6cbb",
   "metadata": {},
   "source": [
    "**Question 2.** Sentiment Analysis\n",
    "\n",
    "We will implement a Recurrent Neural Network (RNN) to perform sentiment analysis on the [IMDB movie review](https://huggingface.co/datasets/stanfordnlp/imdb) dataset, classifying reviews as either positive or negative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fb2c8-b260-44bd-834c-96879a03cc6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.625993900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datasets import load_dataset # \n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3236fa-fdd3-4058-b0cf-1d402ee0d93f",
   "metadata": {},
   "source": [
    "Load the dataset and prepare the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddd8ea-99eb-4c66-a94f-2f645e561c55",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.627485600Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "train_data = imdb['train'][\"text\"]\n",
    "train_labels = imdb[\"train\"][\"label\"]\n",
    "test_data = imdb['test'][\"text\"]\n",
    "test_labels = imdb[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485318-ba3c-4a8e-a032-5cb069bda87f",
   "metadata": {},
   "source": [
    "Preprocess the data by lowercasing, removing punctuation, special characters, and stop words, and tokenizing the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338899e-9a3a-4b22-b30a-e2d347bc9f52",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.628520200Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_texts(texts, stop_words):\n",
    "    \n",
    "    # Complete the implementation\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "train_corpus = preprocess_texts(train_data, stop_words)\n",
    "test_corpus = preprocess_texts(test_data, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045de26-465b-42d9-bf44-fff4f25b0fb4",
   "metadata": {},
   "source": [
    "Map the words to integers and pad the sequences to ensure fixed-length inputs.\n",
    "\n",
    "**Note:** Indexing can be started from $1$ and $0$ can be used for tokens that do not appear in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44263505-b5c2-4e96-a998-7fb9fca63bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:18.630774900Z",
     "start_time": "2024-12-02T14:15:18.630774900Z"
    }
   },
   "outputs": [],
   "source": [
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter([word for text in train_corpus+test_corpus for word in text])\n",
    "# Define the vocabulary\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "# 0 might be used for padding so start from 1.\n",
    "word2int = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "\n",
    "print(\"Vocab size:\", len(word2int))\n",
    "print(\"Training set:\", len(train_data))\n",
    "print(\"Most frequent 5 words:\", vocab[:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b67830-016c-4a98-8fe4-05cc2dedcdf9",
   "metadata": {},
   "source": [
    "Convert word sequences to integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55498765-a299-48c5-a4be-acea7f7f42e8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.630774900Z"
    }
   },
   "outputs": [],
   "source": [
    "train_int_corpus = [[word2int[word] for word in text] for text in train_corpus]\n",
    "test_int_corpus = [[word2int[word] for word in text] for text in test_corpus]\n",
    "\n",
    "def add_padding(corpus, max_length):\n",
    "\n",
    "    # Complete the implementation\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define the maximum length for each input text.\n",
    "# Discard the remaining part of the texts if they are longer than the threshold value.\n",
    "max_length = 200\n",
    "train_int_corpus = add_padding(train_int_corpus, max_length=max_length)\n",
    "test_int_corpus = add_padding(test_int_corpus, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fc7db-777a-4054-bf8d-2fcb9fcfe0d3",
   "metadata": {},
   "source": [
    "Implement a RNN model in PyTorch, including:\n",
    "- an embedding layer to convert words into dense vector representations.\n",
    "- a recurrent layer to capture sequential patterns.\n",
    "- a fully connected output layer for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72a3e3-9499-4dc0-886f-ece2b431f343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:18.636283600Z",
     "start_time": "2024-12-02T14:15:18.630774900Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c342c4-6c12-4ab4-a267-22c1683beb75",
   "metadata": {},
   "source": [
    "Set the model and optimization hyperparameters, and implement the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adb372-75a6-4662-a7fa-188152943b8a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.630774900Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = \n",
    "hidden_size = \n",
    "output_size = \n",
    "batch_size = 64\n",
    "lr = \n",
    "epochs_num = \n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = SentimentRNN(vocab_size, embed_size, hidden_size, output_size)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Construct data loaders\n",
    "train_int_corpus = torch.as_tensor(train_int_corpus)\n",
    "test_int_corpus = torch.as_tensor(test_int_corpus)\n",
    "\n",
    "train_labels = torch.as_tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.as_tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(train_int_corpus, train_labels)\n",
    "test_data = TensorDataset(test_int_corpus, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bea636-bd9a-40f4-9745-c63315a85a3f",
   "metadata": {},
   "source": [
    "Define the evaluation function that can be used to compute the loss and to count the number of correct predictions for each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2baf4ce-776a-4904-a25a-dafcc1293500",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-02T14:15:18.636283600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def batch_evaluate(inputs, labels, model, loss_function):\n",
    "    \n",
    "    \n",
    "    return correct, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9040a4-a98e-4a7c-a678-56b4eebf3d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T14:15:18.684001600Z",
     "start_time": "2024-12-02T14:15:18.636283600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the platform that we will use for training.\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "device = torch.device('mps') \n",
    "\n",
    "# Move the model parameters\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, train_total_correct, total_count = 0, 0, 0\n",
    "    for batch in tqdm(train_loader, desc=\"Batch:\"):\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_inputs, train_labels = batch\n",
    "        train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "        \n",
    "        batch_correct, batch_loss = batch_evaluate(train_inputs, train_labels, model, loss_function)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        train_total_correct += batch_correct\n",
    "        \n",
    "        total_count += len(train_labels)\n",
    "        \n",
    "    accuracy = train_total_correct / total_count\n",
    "    print(f\"Epoch {epoch+1}/{epochs_num}, Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluate test\n",
    "    test_loss, test_total_correct, total_count = 0, 0, 0\n",
    "    for batch in tqdm(test_loader, desc=\"Test Batch:\"):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            test_inputs, test_labels = batch\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "            \n",
    "            batch_correct, batch_loss = batch_evaluate(test_inputs, test_labels, model, loss_function)\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            test_total_correct += batch_correct\n",
    "            total_count += len(test_labels)\n",
    "        \n",
    "    print(f\"Test Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Transfer the model back to the cpu.\n",
    "model.to('cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
