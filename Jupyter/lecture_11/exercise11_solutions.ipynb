{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f888e6-c57e-4bb1-9cae-9a87b449d48d",
   "metadata": {},
   "source": [
    "### Web Intelligence - Exercise 11\n",
    "\n",
    "In this exercise, we will explore the core principles and practical implementations of Recurrent Neural Networks (RNNs), one of the fundamental architectures for sequential data modeling. We will begin by building a simple RNN to understand its mechanics, including how hidden states evolve and how sequential dependencies are captured. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e06c9a-3a14-4c49-ad7f-57dac2ca9277",
   "metadata": {},
   "source": [
    "**Question 1.** Classifying Names with a Character-Level RNN\n",
    "\n",
    "We will implement a character-level Recurrent Neural Network (RNN) to classify names by their language of origin in PyTorch. A dataset containing names from different languages can be downloaded from [here](https://download.pytorch.org/tutorial/data.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1158f0-da38-4cf1-98e7-7bef4191a3d3",
   "metadata": {},
   "source": [
    "Import the required packages and set the data folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66393912-c6d5-4b30-baa1-935928397eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:39:34.879468700Z",
     "start_time": "2025-01-12T10:39:27.710511300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the data folder path\n",
    "data_folder = \"./data/names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2ceb0bd-260b-4ec6-a134-f29f42b28fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:03:38.473033600Z",
     "start_time": "2025-01-11T22:03:38.460031500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabic.', 'Chinese.', 'Czech.', 'Dutch.', 'English.', 'French.', 'German.', 'Greek.', 'Irish.', 'Italian.', 'Japanese.', 'Korean.', 'Polish.', 'Portuguese.', 'Russian.', 'Scottish.', 'Spanish.', 'Vietnamese.']\n"
     ]
    }
   ],
   "source": [
    "# Print possible languages\n",
    "possible_languages = [filename[:-3] for filename in os.listdir(data_folder)]\n",
    "print(possible_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27db7918-e4ff-460d-8633-a9d1e0875521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:07:06.470808600Z",
     "start_time": "2025-01-11T22:07:06.441156500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose languages and read the files.\n",
    "languages = [\"Spanish\", \"French\"]\n",
    "# Set the maximum length of names (discard names whose length larger than the threshold)\n",
    "max_name_length = 15\n",
    "\n",
    "\n",
    "# Read the names stored in the language files.\n",
    "names = []\n",
    "for language in languages:\n",
    "    file_path = os.path.join(data_folder, f\"{language}.txt\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        current_language_names = [line.strip() for line in f if len(line) <= max_name_length]\n",
    "    names.append(current_language_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba99ca-7019-4043-ba62-2ed06081065a",
   "metadata": {},
   "source": [
    "Map letters to character indices and represent names and letters as one-hot encodings.\n",
    "\n",
    "**Note:** Indexing can be started from $1$ and $0$ can be used for unknown characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dca1abe-7c84-48ec-86ff-a7978111f49c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:07:08.121199500Z",
     "start_time": "2025-01-11T22:07:08.103142600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map letters to indices. Note that for unknown characters, we can use index 0.\n",
    "letter2idx = {letter: idx+1 for idx, letter in enumerate(string.ascii_letters)}\n",
    "\n",
    "letters_num = len(letter2idx) + 1 # +1 for unknown letters\n",
    "\n",
    "# Map given letter to one-hot encoding\n",
    "def letter2onehot(letter):\n",
    "    one_hot_vect = torch.zeros(1, letters_num)\n",
    "    idx = letter2idx.get(letter, 0)\n",
    "    if idx != 0:\n",
    "        one_hot_vect[0][idx] = 1\n",
    "    return one_hot_vect\n",
    "\n",
    "# Convert the \n",
    "def name2onehot(name):\n",
    "    assert len(name) < max_name_length, f\"Given name is longer than {max_name_length}!\"\n",
    "    one_hot_enc = torch.zeros(max_name_length, letters_num)\n",
    "    for pos, letter in enumerate(name):\n",
    "        # if letter not in letter2idx:\n",
    "        #     raise ValueError(f\"Letter {letter} = {name}\")\n",
    "        one_hot_enc[-len(name)+pos:, :] = letter2onehot(letter)\n",
    "    return one_hot_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2344a845-bcbf-4366-8534-c74701370b32",
   "metadata": {},
   "source": [
    "Define the dataset tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ec39a38-5b2e-4211-8e9e-c7b6a5c062dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:07:27.503096400Z",
     "start_time": "2025-01-11T22:07:27.434315Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = torch.stack(\n",
    "    [name2onehot(name) for language_names in names for name in language_names]\n",
    ")\n",
    "train_labels = torch.as_tensor(\n",
    "    [label for label, language_names in enumerate(names) for _ in language_names],\n",
    "    dtype=torch.long\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a985339-6ce4-4ca8-8f6e-a644e08408b9",
   "metadata": {},
   "source": [
    "Define an RNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb5c538-a9f2-4e81-a0ca-eb940f0c7242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:07:29.842704700Z",
     "start_time": "2025-01-11T22:07:29.826701800Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = torch.nn.functional.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac8b30-76d9-4532-b07c-4a604cca04f3",
   "metadata": {},
   "source": [
    "For training, define the model and optimization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e689f38-b862-4c91-89f3-2d0f147f91ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:07:42.217991200Z",
     "start_time": "2025-01-11T22:07:32.312143700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of languages\n",
    "class_num = len(languages)\n",
    "# Define the model hyper-parameters\n",
    "n_hidden = 128\n",
    "# Define the optimization parameter\n",
    "lr = 0.01\n",
    "epochs_num = 100\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = RNN(letters_num, n_hidden, class_num)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2363a-b913-4b8c-930a-097d256dec13",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4efff2-2906-4fd7-8c09-dfdd75a395a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T22:08:40.174412400Z",
     "start_time": "2025-01-11T22:08:30.081969500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e3153f50c824b16b46c536c4da855ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Current epoch::   0%|          | 0/575 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9313141c398c42938add99927dcd4d08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5951\n"
     ]
    },
    {
     "data": {
      "text/plain": "Current epoch::   0%|          | 0/575 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7c39b51e64048b596d823142a2e8e04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.4312\n"
     ]
    },
    {
     "data": {
      "text/plain": "Current epoch::   0%|          | 0/575 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce9f3e8401b7401d89c68e439cc72bf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.3577\n"
     ]
    },
    {
     "data": {
      "text/plain": "Current epoch::   0%|          | 0/575 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0b0f1e755ba41fcb2332db9cd08429f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.3207\n"
     ]
    },
    {
     "data": {
      "text/plain": "Current epoch::   0%|          | 0/575 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c9752eb39b34a05b37d13f91e424791"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m letter_enc \u001B[38;5;129;01min\u001B[39;00m input_mat:\n\u001B[1;32m---> 25\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mletter_enc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[0;32m     28\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(output, label)\n",
      "File \u001B[1;32mC:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[21], line 13\u001B[0m, in \u001B[0;36mRNN.forward\u001B[1;34m(self, input, hidden)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, hidden):\n\u001B[1;32m---> 13\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mtanh(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi2h\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2h(hidden))\n\u001B[0;32m     14\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh2o(hidden)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output, hidden\n",
      "File \u001B[1;32mC:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mC:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Set the platform that we will use for training.\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "# device = torch.device('mps') \n",
    "device = torch.device('cpu') \n",
    "\n",
    "# Move the model parameters\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for idx in tqdm(torch.randperm(train_data.shape[0]), desc=\"Current epoch:\"):\n",
    "        \n",
    "        input_mat = train_data[idx].to(device)\n",
    "        label = train_labels[idx].to(device).unsqueeze(0)\n",
    "    \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Initialize the hidden state of the model\n",
    "        hidden = model.initHidden().to(device)\n",
    "        # \n",
    "        for letter_enc in input_mat:\n",
    "            output, hidden = model(letter_enc, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        # optimizer.step()\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-lr)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/train_data.shape[0]:.4f}\")  \n",
    "        \n",
    "# Transfer the model back to the cpu.\n",
    "# model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc4fd6-9177-4fd3-9cf9-cf218e09e387",
   "metadata": {},
   "source": [
    "Implement an evaluation to measure accuracy on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57161337-f198-431f-8c6d-8a3d2e5dacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[296.,   2.],\n",
      "        [  8., 269.]])\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracies = torch.zeros(class_num, class_num)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for class_idx, language_names in enumerate(names):\n",
    "        success_count = 0\n",
    "        for name in language_names:\n",
    "            \n",
    "            hidden = model.initHidden()\n",
    "            for letter_enc in name2onehot(name):\n",
    "                output, hidden = model(letter_enc, hidden)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)        \n",
    "            accuracies[class_idx][pred] += 1\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fa275-4003-4ef8-961c-047929db6cbb",
   "metadata": {},
   "source": [
    "**Question 2.** Sentiment Analysis\n",
    "\n",
    "We will implement a Recurrent Neural Network (RNN) to perform sentiment analysis on the [IMDB movie review](https://huggingface.co/datasets/stanfordnlp/imdb) dataset, classifying reviews as either positive or negative. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96fb2c8-b260-44bd-834c-96879a03cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datasets import load_dataset # \n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3236fa-fdd3-4058-b0cf-1d402ee0d93f",
   "metadata": {},
   "source": [
    "Load the dataset and prepare the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ddd8ea-99eb-4c66-a94f-2f645e561c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "train_data = imdb['train'][\"text\"]\n",
    "train_labels = imdb[\"train\"][\"label\"]\n",
    "test_data = imdb['test'][\"text\"]\n",
    "test_labels = imdb[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485318-ba3c-4a8e-a032-5cb069bda87f",
   "metadata": {},
   "source": [
    "Preprocess the data by lowercasing, removing punctuation, special characters, and stop words, and tokenizing the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c338899e-9a3a-4b22-b30a-e2d347bc9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be149614ee042059e8a5ddfc6d8073a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-processing text:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a173db48baea44039768a48420965043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-processing text:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_texts(texts, stop_words):\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in tqdm(texts, desc=\"Pre-processing text\"):\n",
    "        \n",
    "        # 1. Lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 3. Remove punctuation and special characters\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        \n",
    "        # Remove html tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        text = re.sub(clean, '', text)\n",
    "        \n",
    "        # 4. Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 5. Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # 6. Lemmatize the tokens \n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        # Join tokens back into a single string\n",
    "        cleaned_texts.append(words)\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "train_corpus = preprocess_texts(train_data, stop_words)\n",
    "test_corpus = preprocess_texts(test_data, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045de26-465b-42d9-bf44-fff4f25b0fb4",
   "metadata": {},
   "source": [
    "Map the words to integers and pad the sequences to ensure fixed-length inputs.\n",
    "\n",
    "**Note:** Indexing can be started from $1$ and $0$ can be used for tokens that do not appear in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44263505-b5c2-4e96-a998-7fb9fca63bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 164578\n",
      "Training set: 25000\n",
      "Most frequent 5 words: ['br', 'movie', 'film', 'one', 'like']\n"
     ]
    }
   ],
   "source": [
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter([word for text in train_corpus+test_corpus for word in text])\n",
    "# Define the vocabulary\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "# 0 might be used for padding so start from 1.\n",
    "word2int = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "\n",
    "print(\"Vocab size:\", len(word2int))\n",
    "print(\"Training set:\", len(train_data))\n",
    "print(\"Most frequent 5 words:\", vocab[:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b67830-016c-4a98-8fe4-05cc2dedcdf9",
   "metadata": {},
   "source": [
    "Convert word sequences to integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55498765-a299-48c5-a4be-acea7f7f42e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67633719833f413e82e717d78812691c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding pad: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f622971ad8324a12b4e643dbc85ee771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding pad: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_int_corpus = [[word2int[word] for word in text] for text in train_corpus]\n",
    "test_int_corpus = [[word2int[word] for word in text] for text in test_corpus]\n",
    "\n",
    "def add_padding(corpus, max_length):\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    output = torch.zeros(size=(len(corpus), max_length), dtype=int)\n",
    "\n",
    "    # Add padding and discard the remaining part if it is longer than the 'max_length'\n",
    "    for idx, text in tqdm(enumerate(corpus), desc=\"Adding pad\"):\n",
    "        output[idx, -len(text):] = torch.as_tensor(text, dtype=int)[:max_length]\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define the maximum length for each input text.\n",
    "# Discard the remaining part of the texts if they are longer than the threshold value.\n",
    "max_length = 200\n",
    "train_int_corpus = add_padding(train_int_corpus, max_length=max_length)\n",
    "test_int_corpus = add_padding(test_int_corpus, max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4982137-69cb-4787-900d-d4fdc6e9da3e",
   "metadata": {},
   "source": [
    "Implement a RNN model in PyTorch, including:\n",
    "- an embedding layer to convert words into dense vector representations.\n",
    "- a recurrent layer to capture sequential patterns.\n",
    "- a fully connected output layer for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c13b033-0720-4a7b-b094-3cc7cf9de3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = torch.nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # Use the last hidden state\n",
    "        logits = self.fc(hidden.squeeze(0))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e038c43-5cf9-42c3-b82d-92a5337bc1ff",
   "metadata": {},
   "source": [
    "Set the model and optimization hyperparameters, and implement the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "773ecab5-430e-4aac-834d-5d33dafc24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 100\n",
    "hidden_size = 50\n",
    "output_size = 2\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "epochs_num = 10\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = SentimentRNN(vocab_size, embed_size, hidden_size, output_size)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Construct data loaders\n",
    "train_int_corpus = torch.as_tensor(train_int_corpus)\n",
    "test_int_corpus = torch.as_tensor(test_int_corpus)\n",
    "\n",
    "train_labels = torch.as_tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.as_tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(train_int_corpus, train_labels)\n",
    "test_data = TensorDataset(test_int_corpus, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d1510-843e-4f7a-ae6e-a2dc367d6a4e",
   "metadata": {},
   "source": [
    "Define the evaluation function that can be used to compute the loss and to count the number of correct predictions for each mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3639a79b-b898-4810-908c-2c512558b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def batch_evaluate(inputs, labels, model, loss_function):\n",
    "    correct = 0\n",
    "    \n",
    "    logits = model(inputs)\n",
    "    loss = loss_function(logits, labels)\n",
    "    \n",
    "    preds = logits.argmax(dim=1)\n",
    "    correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return correct, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e17309e0-7c99-47fb-8639-faa6cc19a0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01d1407fa4c406fafc4a4544f63004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f5b39c7504b30a2e4a3ad78e7ced2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 258.0152, Accuracy: 0.6076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5dd39c166e4f07826098d3eafdb297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 258.0152, Accuracy: 0.6076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ec426549e6422da5b6f508a599e7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 210.2010, Accuracy: 0.7414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce55fcd1f2b34a0d81540c49ed99bd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 210.2010, Accuracy: 0.7414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104c1fb2b9e84f76ba0a6924aaf22282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 185.3649, Accuracy: 0.7794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b6285a1844413e9fe31859c874a88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 185.3649, Accuracy: 0.7794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5452d51eb18f46a784aeb81aa65b2c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 180.2875, Accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0614991e45d3460eb201cfe0072fc651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 180.2875, Accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db421e06d84446e882ce37fee98e599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 155.0892, Accuracy: 0.8257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035eb5babce0430c923fff8aaf6adfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 155.0892, Accuracy: 0.8257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c47b47ade054e3cad1282232dd08e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 138.8202, Accuracy: 0.8509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28e8542096146679298b5ad0191d565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 138.8202, Accuracy: 0.8509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fa3514afc4446eaa888bdf1784939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 132.1910, Accuracy: 0.8593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc483fd5295b492dbf962f1c00aa5824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 132.1910, Accuracy: 0.8593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cc43dfac174a6392d9623d02706953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 124.9832, Accuracy: 0.8686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820cd31352d944c1989fb87052452d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 124.9832, Accuracy: 0.8686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83ba1fcd61d4088af18c0a79ce4ec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 118.4366, Accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8203512f78c84acc9198fc886dbabe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 118.4366, Accuracy: 0.8762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af7ef83b81647fda5a7960147941731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 112.4382, Accuracy: 0.8829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eb46f002f6434e9a3b8f3e0ec4ea5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Batch::   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 112.4382, Accuracy: 0.8829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(164578, 100)\n",
       "  (rnn): RNN(100, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the platform that we will use for training.\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "device = torch.device('mps') \n",
    "\n",
    "# Move the model parameters\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss, train_total_correct, total_count = 0, 0, 0\n",
    "    for batch in tqdm(train_loader, desc=\"Batch:\"):\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_inputs, train_labels = batch\n",
    "        train_inputs, train_labels = train_inputs.to(device), train_labels.to(device)\n",
    "        \n",
    "        batch_correct, batch_loss = batch_evaluate(train_inputs, train_labels, model, loss_function)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.item()\n",
    "        train_total_correct += batch_correct\n",
    "        \n",
    "        total_count += len(train_labels)\n",
    "        \n",
    "    accuracy = train_total_correct / total_count\n",
    "    print(f\"Epoch {epoch+1}/{epochs_num}, Train Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "    # Evaluate test\n",
    "    test_loss, test_total_correct, total_count = 0, 0, 0\n",
    "    for batch in tqdm(test_loader, desc=\"Test Batch:\"):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            test_inputs, test_labels = batch\n",
    "            test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
    "            \n",
    "            batch_correct, batch_loss = batch_evaluate(test_inputs, test_labels, model, loss_function)\n",
    "        \n",
    "            test_loss += batch_loss.item()\n",
    "            test_total_correct += batch_correct\n",
    "            total_count += len(test_labels)\n",
    "        \n",
    "    print(f\"Test Loss: {train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Transfer the model back to the cpu.\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0128fe-22fa-4127-9f5f-3ca0542bc83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
