{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a718ed67-f8a8-4533-a234-f0b456997d89",
   "metadata": {},
   "source": [
    "### Web Intelligence - Exercise 12\n",
    "\n",
    "In this last exercise, we will explore the foundational principles and practical applications of the *Transformer* encoder, a key building block in modern natural language processing (NLP). The Transformer architecture, introduced in the landmark paper \"*Attention is All You Need*\", has become a cornerstone of NLP due to its ability to efficiently model long-range dependencies and perform computations in parallel.\n",
    "\n",
    "\n",
    "Our focus will be on implementing the Transformer encoder for a sentiment analysis task using the [IMDB movie review](https://huggingface.co/datasets/stanfordnlp/imdb) dataset. This task will help you understand the core components of the Transformer encoder, including the self-attention mechanism, positional encodings, and feedforward layers, and how these contribute to processing sequential data effectively. To support you in this process, a Jupyter Notebook file is provided, guiding you through data preprocessing, model construction, training, and evaluation step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e81066-32f2-4106-bc93-fb405d6add2b",
   "metadata": {},
   "source": [
    "**Question 1.** Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d3759b-74a2-405f-bb0b-b665e1793508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:49:26.911823300Z",
     "start_time": "2025-01-12T10:49:12.115085400Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from datasets import load_dataset # \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da335108-84c7-41aa-83e0-e40e84a4117b",
   "metadata": {},
   "source": [
    "Load the dataset and prepare the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb4ac02-2d74-4562-8aff-b010b25c9868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:49:34.373093800Z",
     "start_time": "2025-01-12T10:49:26.916797100Z"
    }
   },
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "train_data = imdb['train'][\"text\"]\n",
    "train_labels = imdb[\"train\"][\"label\"]\n",
    "test_data = imdb['test'][\"text\"]\n",
    "test_labels = imdb[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bcbff-4201-459c-9961-6958c9beeede",
   "metadata": {},
   "source": [
    "Preprocess the data by lowercasing, removing punctuation, special characters, and stop words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03131041-7ed2-45f0-aaeb-df77d369f154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T10:50:36.352007900Z",
     "start_time": "2025-01-12T10:49:59.114843800Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocess raw text\n",
    "def clean_data(texts):\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        \n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        \n",
    "        # Remove html tags\n",
    "        clean = re.compile('<.*?>')\n",
    "        text = re.sub(clean, '', text)\n",
    "        \n",
    "        # Tokenize the text\n",
    "        text = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        text = [word for word in text if word not in stop_words]\n",
    "        \n",
    "        # Lemmatize the tokens \n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "        \n",
    "        # Join tokens back into a single string\n",
    "        cleaned_texts.append(' '.join(text))\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "train_data = clean_data(train_data)\n",
    "test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518972c-1a1d-43d5-bee4-42f4605d7ba4",
   "metadata": {},
   "source": [
    "Build a vocabulary and map each word to a unique integer value.\n",
    "\n",
    "**Note**.  We can only consider the top-$K$ most common words in the corpus so we can define an additional token (i.e. \"$<$unk$>$\") to indicate these \"unknown\" or discarded words in the dataset. We will also apply the padding to have equal-length sequences. In other words, if a given input sequence is shorter than the expected length, then we will add additional tokens to reach the desired sequence size. For this purpose, we can define another token (i.e., \"$<$pad$>$\") to represent these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00134ac1-4312-42b0-a391-4d0abbb31cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Tokenizer and Vocabulary\n",
    "def tokenize_and_build_vocab(texts, vocab_size=20000):\n",
    "    # Define the counter\n",
    "    counter = Counter(word for text in texts for word in word_tokenize(text))\n",
    "    # Get the most common 'vocab_size'-2 words and reserve the two slots for unknown(<unk>) and padding (<pad>) tokens.\n",
    "    most_common = counter.most_common(vocab_size - 2)\n",
    "    # Define the vocabulary mapping tokens to indices\n",
    "    vocab = {word: i + 2 for i, (word, _) in enumerate(most_common)}\n",
    "    vocab[\"<unk>\"] = 0 # token for the unknown words\n",
    "    vocab[\"<pad>\"] = 1 # token for the padding\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = tokenize_and_build_vocab(train_data, vocab_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa20c8-4745-4400-8e4d-c32ed235e557",
   "metadata": {},
   "source": [
    "Convert the dataset into sequences of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14cbc2b-dc39-4099-b4ff-9600144d8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into sequences of token indices.\n",
    "def token2indices(texts, vocab):\n",
    "    '''\n",
    "    Converts a list of texts to token indices. If tokens are not in vocab, use the <unk> token.\n",
    "    :param texts: \n",
    "    :param vocab: \n",
    "    :return: \n",
    "    '''\n",
    "    integer_sequences = []\n",
    "    for text in texts:\n",
    "        current_seq = [vocab.get(token, vocab[\"<unk>\"]) for token in word_tokenize(text)]\n",
    "        integer_sequences.append(current_seq)\n",
    "            \n",
    "    return integer_sequences\n",
    "\n",
    "train_int_data = token2indices(train_data, vocab)\n",
    "test_int_data = token2indices(test_data, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dcaa60-49ae-4d78-bcb0-1de25796c019",
   "metadata": {},
   "source": [
    "Define the dataset and data loaders required for the training procedure as we did in the previous exercises. We will also implement a function to pad/truncate sequences to a fixed length and set the *collate_fn* parameter of *PyTorch*'s *DataLoader* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f76e91-8162-424a-8216-833b40b3d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 500\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, label):\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.texts[index], self.label[index]\n",
    "        \n",
    "\n",
    "# Data collator for padding\n",
    "def collate_batch(batch, max_len):\n",
    "    texts, labels = zip(*batch)\n",
    "    padded_sequence = torch.as_tensor(\n",
    "        [text[:max_len] + [vocab[\"<pad>\"]] * (max_len - len(text)) for text in texts], dtype=torch.long\n",
    "    )\n",
    "    labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return padded_sequence, labels\n",
    "\n",
    "# Define the datasets\n",
    "train_dataset = IMDBDataset(train_int_data, train_labels)\n",
    "test_dataset = IMDBDataset(test_int_data, test_labels)\n",
    "\n",
    "# Define the data loader\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=lambda batch: collate_batch(batch, max_len=seq_len))\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, collate_fn=lambda batch: collate_batch(batch, max_len=seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499780f-c6f5-4a32-a9e8-d507c467c796",
   "metadata": {},
   "source": [
    "Implement a positional encoding function to inject sequence order information into the token embeddings. Use sinusoidal or learned positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4a85e3-f579-4995-8182-48b8d7a24cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    '''\n",
    "    Positional encoding module\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model_dim, seq_len, device='cpu'):\n",
    "        \"\"\"\n",
    "        theta = p / 10000^(2d/model_dim) = p * exp(-(2d/model_dim)*log(10000))\n",
    "        pe[:, 0::2] = sin(theta) and pe[:, 1::2] = cos(theta)\n",
    "        \n",
    "        :param model_dim: model dimension\n",
    "        :param seq_len: sequence length\n",
    "        :param device: device (default: cpu)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "         \n",
    "        theta_num_term = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        theta_deno_term = torch.exp( \n",
    "            (-torch.arange(0, model_dim, 2, dtype=torch.float, device=device) / model_dim) * math.log(10000.0) \n",
    "        )\n",
    "        theta = theta_num_term * theta_deno_term\n",
    "        \n",
    "        self.pos_emb = torch.zeros(seq_len, model_dim, device=device)\n",
    "        self.pos_emb[:, 0::2] = torch.sin(theta)\n",
    "        self.pos_emb[:, 1::2] = torch.cos(theta)\n",
    "\n",
    "    def forward(self, current_emb):\n",
    "        \"\"\"\n",
    "        Adds the positional encoding to the input embedding tensor\n",
    "        :param current_emb: a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return current_emb + self.pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122684ad-778f-4d0d-ae4d-104347f5c7b4",
   "metadata": {},
   "source": [
    "- Calculate **query (Q)**, **key (K)**, and **value (V)** matrices by applying learned linear transformations to the input embeddings.\n",
    "    \n",
    "- Compute scaled dot-product attention for each token: \n",
    "$$\n",
    "    Attention(Q,K,V) = Softmax\\left( \\frac{QK^\\top}{\\sqrt{d_K}} \\right)V\n",
    "$$\n",
    "where $d_K$ is the dimensionality of the key vectors.\n",
    "\n",
    "- Implement the multi-head attention by splitting embeddings into multiple heads, applying the attention mechanism for each head, and concatenating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261f25d8-7590-4f1c-a1ec-b4ab52e06dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Scaled Dot-Product Attention\n",
    "    The forward method returns the scaled dot-product attention\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Implements Scaled Dot-Product Attention\n",
    "        :param query: a tensor of shape (batch_size, heads_num, seq_len, key_dim)\n",
    "        :param key: a tensor of shape (batch_size, heads_num, seq_len, key_dim)\n",
    "        :param value: a tensor of shape (batch_size, heads_num, seq_len, key_dim)\n",
    "        :return: dot-product and attention\n",
    "        \"\"\"\n",
    "        key_dim = key.shape[-1]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(key_dim)\n",
    "        \n",
    "        # Apply the softmax function    \n",
    "        attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        return torch.matmul(attn, value), attn\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Set the number of heads\n",
    "        self.num_heads = num_heads\n",
    "        # Define the dimension size of single query, key and value matrices\n",
    "        self.key_dim = model_dim // num_heads\n",
    "        # Define the scaled-dot product attention class\n",
    "        self.sdpa = ScaledDotProductAttention()\n",
    "\n",
    "        # Set the weights for query, key and value\n",
    "        self.w_q = torch.nn.Linear(model_dim, model_dim, device=device)\n",
    "        self.w_k = torch.nn.Linear(model_dim, model_dim, device=device)\n",
    "        self.w_v = torch.nn.Linear(model_dim, model_dim, device=device)\n",
    "        \n",
    "        # Define the linear layer on the top of the concatenation operation\n",
    "        self.linear = torch.nn.Linear(model_dim, model_dim, device=device)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        Implements Scaled Dot-Product Attention for multi-head attention\n",
    "        Instead of concatenating individual heads, we can build tensors in the form of already concatenated\n",
    "        :param query: a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        :param key: a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        :param value: a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the query, key and values\n",
    "        batch_size = query.shape[0]\n",
    "        query = self.w_q(query).view(batch_size, -1, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "        key = self.w_k(key).view(batch_size, -1, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "        value = self.w_v(value).view(batch_size, -1, self.num_heads, self.key_dim).transpose(1, 2)\n",
    "\n",
    "        # Get the scaled dot-products that are of shape (batch_size, heads_num, seq_len, model_dim)\n",
    "        # For the concatenation of multiple heads, we can simply reshape the tensor\n",
    "        scores, _ = self.sdpa(query, key, value)\n",
    "        # Concatenate the heads. Here, we need to call contiguous() to reorganize the order of tensor's elements in memory\n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.key_dim)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(scores)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb003967-7958-4369-b6f7-5d5a8516d00f",
   "metadata": {},
   "source": [
    "Implement the encoder component that consists of the following steps:\n",
    "- A multi-head attention mechanism.\n",
    "- A feedforward network consisting of two linear layers separated by a ReLU activation.\n",
    "- Residual connections around both the attention mechanism and the feedforward network.\n",
    "- Layer normalization after each residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4659a285-9f21-4f77-a064-a7e435838b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, model_dim, heads_num, hidden_dim, dropout=0.1, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Defines the encoder block\n",
    "        :param model_dim: the model dimension\n",
    "        :param heads_num: the number of heads\n",
    "        :param hidden_dim: the dimension of the feedforward network model\n",
    "        :param dropout: the dropout rate (default: 0.1)\n",
    "        :param device: the device (default: cpu)\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        # Define the multi-head attention layer\n",
    "        self.multi_head_attention_layer = MultiHeadAttention(model_dim, heads_num, device=device)\n",
    "        # Define the layer normalization\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(model_dim, device=device)\n",
    "        # Define the feed forward layer\n",
    "        self.feed_forward_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model_dim, hidden_dim, device=device), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(hidden_dim, model_dim, device=device),\n",
    "        ) \n",
    "        # Define the second layer normalization\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(model_dim, device=device)\n",
    "        # Dropout layer\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head attention layer\n",
    "        attention_out = self.multi_head_attention_layer(query=x, key=x, value=x)\n",
    "        # Add & Norm\n",
    "        x2 = self.layer_norm1(x + self.dropout(attention_out))\n",
    "        # Feed forward layer\n",
    "        feed_forward_out = self.feed_forward_layer(x2)\n",
    "        # Add & Norm\n",
    "        output = self.layer_norm2(x2 + self.dropout(feed_forward_out))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ceb35b-069e-4eba-8c69-7c666f78f3af",
   "metadata": {},
   "source": [
    "- Initialize the word embeddings and add position encodings to them.\n",
    "- **Stack Multiple Encoder Blocks.** Arrange multiple encoder blocks sequentially to construct the full encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955407e6-b704-4204-874a-756ef71d555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, heads_num, layers_num, hidden_dim, seq_len, dropout=0.1, device=\"cpu\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: the vocabulary size\n",
    "        :param model_dim: the model dimension\n",
    "        :param heads_num: the number of heads\n",
    "        :param layers_num: the number of layers\n",
    "        :param hidden_dim: the dimension of the feedforward network model\n",
    "        :param seq_len: the sequence length\n",
    "        :param dropout: the dropout rate (default: 0.1)\n",
    "        :param device: the device type (default: cpu)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the input embeddings class\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, model_dim, device=device)\n",
    "        # Define the Positional Encoding class\n",
    "        self.pos_encoding = PositionalEncoding(model_dim=model_dim, seq_len=seq_len, device=device)\n",
    "        # Define the encoder layer classes\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            [EncoderBlock(model_dim, heads_num, hidden_dim, dropout, device=device) for _ in range(layers_num)]\n",
    "        )\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the encoder\n",
    "        :param sequences: a tensor storing word indices of shape (batch_size, seq_len)\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the word embeddings, a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        emb = self.embedding(sequences)\n",
    "        # Add position encodings\n",
    "        emb = self.pos_encoding(emb)\n",
    "        \n",
    "        # Apply the encoder for 'layers_num' times\n",
    "        x = emb\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # Output is a tensor of shape (batch_size, seq_len, model_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717c5ed-1c2a-4f5a-a3a0-c2833f67510c",
   "metadata": {},
   "source": [
    "**Building a classifier**\n",
    "- Use a feedforward output layer with a sigmoid activation function to predict the sentiment label (positive or negative) by using the mean of the outputs of the encoder architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f8c2b43-6337-4b61-a03a-4550b432ce17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(20000, 100)\n",
       "    (pos_encoding): PositionalEncoding()\n",
       "    (encoder_layers): ModuleList(\n",
       "      (0-2): 3 x EncoderBlock(\n",
       "        (multi_head_attention_layer): MultiHeadAttention(\n",
       "          (sdpa): ScaledDotProductAttention()\n",
       "          (w_q): Linear(in_features=100, out_features=100, bias=True)\n",
       "          (w_k): Linear(in_features=100, out_features=100, bias=True)\n",
       "          (w_v): Linear(in_features=100, out_features=100, bias=True)\n",
       "          (linear): Linear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward_layer): Sequential(\n",
       "          (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=100, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "heads_num = 2\n",
    "layers_num = 3\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "device = torch.device('cpu') \n",
    "#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "  def __init__(self, vocab_size, model_dim, heads_num, layers_num, hidden_dim, seq_len, dropout, device):\n",
    "      super(Classifier, self).__init__()\n",
    "      \n",
    "      self.encoder = Encoder(vocab_size, model_dim, heads_num, layers_num, hidden_dim, seq_len, dropout, device)\n",
    "      self.fc = torch.nn.Linear(model_dim, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoder_output = self.encoder(x)\n",
    "    \n",
    "    output = encoder_output.mean(dim=1)\n",
    "    output = self.fc(output)\n",
    "    output = torch.nn.functional.sigmoid(output)\n",
    "      \n",
    "    return output\n",
    "  \n",
    "model = Classifier(\n",
    "    len(vocab), model_dim=embedding_dim, \n",
    "    heads_num=heads_num, layers_num=layers_num, \n",
    "    hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "    dropout=dropout, device=device\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e26a2f-c324-4233-aa03-eef680808b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55aa39cf1b9487cb99d4617413376f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d752124681c1408580062f98c2786492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5195837401787339, Accuracy: 0.7304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c3cb2b0ab64a0db85c5ae660a100c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.36916401284887357, Accuracy: 0.8378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07993a898b541c88246a2e4947f4b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.2871998687801154, Accuracy: 0.88076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c83fab2c24d5cb36ed0055a9b37ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.23104342945691797, Accuracy: 0.90868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f09a969cfd446f8acd3f928921c35d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.18619978621535366, Accuracy: 0.9304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7f42684b2347efabe876e0b2808a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.1475690850294898, Accuracy: 0.9478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f9537ae7204cfbb5fb2e8c1125ffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.12227097666903358, Accuracy: 0.95636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49db65bbba41c686664d1967d87828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.10963158288320331, Accuracy: 0.95924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756b5dbfcbcb491abbde65af810bed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.08819135021575539, Accuracy: 0.96932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a0a5c256e3456b8a26506bcb3c5b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.06962468651944505, Accuracy: 0.97548\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import BCELoss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training Setup\n",
    "loss_func = BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, loss_func, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs),desc='Epoch'):\n",
    "        \n",
    "        total_loss, total_count, correct = 0, 0, 0\n",
    "        for texts, labels in tqdm(train_loader, desc='Batch'):\n",
    "            texts, labels = texts.to(device), labels.to(device).view(-1, 1).to(torch.float)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(texts)\n",
    "            loss = loss_func(predictions, labels)\n",
    "            correct += ((predictions > 0.5) == labels).sum().item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            total_count += len(labels)\n",
    "        \n",
    "        accuracy = correct / total_count\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}, Accuracy: {accuracy}\")\n",
    "        \n",
    "train_model(model, train_loader, loss_func, optimizer, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1dce1-f90b-4ae5-9b07-b480b1e7bb84",
   "metadata": {},
   "source": [
    "Implement an evaluation function to measure accuracy on the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd552937-b51c-469c-9d3a-6669f2407a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts, labels = texts.to(device), labels.view(-1, 1)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            \n",
    "            predictions = (predictions > 0.5)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "    print(f\"Accuracy: {correct/total:.2f}\")\n",
    "\n",
    "evaluate_model(model, train_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715801d-7f52-47e7-87c6-4c4c37609f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f7feb-623d-4dc2-b621-08e306009df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
