{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5a9e28-680a-4347-9c2c-34b6bd6d6556",
   "metadata": {},
   "source": [
    "### Web Intelligence - Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af618479-764d-4275-86ae-41a6fcd0859b",
   "metadata": {},
   "source": [
    "In this exercise, we will explore several prominent techniques for learning word representations. First, we will implement a matrix decomposition-based word embedding model, **Latent Semantic Analysis (LSA)**. These models leverage matrix factorization to capture semantic relationships in text, providing an introduction to the underlying principles of word representation.\n",
    "\n",
    "Next, we will implement a neural-based approach, the **SkipGram** model of the **Word2Vec** framework. The SkipGram architecture is designed to predict surrounding context words given a target word, effectively learning word representations that capture meaningful relationships between words in a corpus.\n",
    "\n",
    "We will work on the [CMU Book Summary](https://www.cs.cmu.edu/~dbamman/booksummaries.html) dataset that consists of plot summaries for $16,559$ books extracted from Wikipedia. Throughout the exercise, we will analyze and compare these methods, discussing their advantages, limitations, and practical applications in natural language processing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64444b45-6407-4a99-aaa4-48eb4fc77ab8",
   "metadata": {},
   "source": [
    "**Question 1.** In this exercise, we will implement the Latent Semantic Analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff37314-ed5b-4bc0-9d65-9a7532fbf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Preprocessing packages\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29251f-9d2d-4e95-89dd-b2601ef9a5b9",
   "metadata": {},
   "source": [
    "Load the dataset and preprocess the book summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016111-703e-455f-9bb2-57a9cf5b4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CMU Book Summary Corpus dataset and get all the summaries.\n",
    "ld = load_dataset(\"textminr/cmu-book-summaries\")['train']\n",
    "summaries = [data['summary'] for data in ld]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d66ecc6aeec364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:43:59.164896Z",
     "start_time": "2024-11-17T19:43:38.059426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Complete the following preprocessing steps:\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_texts(texts, stop_words):\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in tqdm(texts, desc=\"Pre-processing text\"):\n",
    "        \n",
    "        # 1. Lowercase the text\n",
    "        text = \n",
    "        \n",
    "        # 3. Remove punctuation and special characters\n",
    "        text = \n",
    "        \n",
    "        # 4. Tokenize the text\n",
    "        words = \n",
    "        \n",
    "        # 5. Remove stopwords\n",
    "        words = \n",
    "        \n",
    "        # 6. Lemmatize the tokens \n",
    "        words = \n",
    "        \n",
    "        # Join tokens back into a single string\n",
    "        cleaned_texts.append(\" \".join(words))\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "# Run the preprocessing function on the summaries\n",
    "corpus = preprocess_texts(summaries, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ce7808d87eda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:43:59.169265Z",
     "start_time": "2024-11-17T19:43:59.167070Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# You can compare the original and preprocessed summaries to see the difference.\n",
    "print(f\"Original: {summaries[0][:100]}\")\n",
    "print(f\"Preprocessed: {corpus[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55aea1-60e1-4599-8b5d-329f9b056c52",
   "metadata": {},
   "source": [
    "Construct the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0781140bd436c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:00.831078Z",
     "start_time": "2024-11-17T19:43:59.170141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: you can only consider the top 'N' words ordered by term frequency across the corpus.\n",
    "\n",
    "vocab_size = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7add3-344f-483d-b89c-7304d62fa377",
   "metadata": {},
   "source": [
    "Construct the Term Frequency-Inverse Document Frequency (TF-IDF) Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0cdbde08a54f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:00.913399Z",
     "start_time": "2024-11-17T19:44:00.832036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit and transform the document-term matrix to the TF-IDF matrix\n",
    "\n",
    "tfidf_matrix = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117d862-2934-44b2-9ba8-11bdfaa7f20a",
   "metadata": {},
   "source": [
    "Perfom Singular Value Decomposition (SVD) and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228affb9bf0d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:01.335336Z",
     "start_time": "2024-11-17T19:44:00.914167Z"
    }
   },
   "outputs": [],
   "source": [
    "k = \n",
    "\n",
    "U, S, Vh = \n",
    "\n",
    "# Print the singular values\n",
    "print(\"\\nTop k- singular values:\\n\", S)\n",
    "\n",
    "# Define the word embeddings\n",
    "# Note that the columns of SVh.T define the word embeddings, i.e. rows of the VhS\n",
    "# SVh.T is also equal to U^TX so the rows of (X^TU) also correspond the same word embeddings.\n",
    "word_embeddings = np.dot(np.diag(S), Vh).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d9622-4ff7-4016-8a9c-d0a7e6aade9e",
   "metadata": {},
   "source": [
    "Generate Word Embeddings and Analyze Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac9536b614fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.019995Z",
     "start_time": "2024-11-17T19:44:01.343915Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'man', 'woman', 'boy', 'daughter', \n",
    "    #'dog', 'cat', 'horse', 'elephant', 'fish', 'bird', 'lizard', 'snake',\n",
    "    #'doctor', 'nurse', 'scientist', 'teacher', 'engineer', 'artist', 'musician', 'writer',\n",
    "    #'denmark', 'sweden', 'france', 'germany', 'spain', 'italy', 'portugal', 'turkey',\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa279de7b9b5671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.033215Z",
     "start_time": "2024-11-17T19:44:32.021217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing similarity between words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_terms(word1, word2, word_embeddings_df):\n",
    "    \n",
    "    return \n",
    "\n",
    "# Find top 5 similar words for a given word\n",
    "def top_similar_words(word, word_embeddings_df, top_n=5):\n",
    "\n",
    "    return \n",
    "\n",
    "word_embeddings_df = pd.DataFrame(word_embeddings, index=terms)\n",
    "\n",
    "word1, word2 = \"king\", \"queen\"\n",
    "# Test cosine similarity function\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity_terms(word1, word2, word_embeddings_df)}\")\n",
    "word = \"king\"\n",
    "print(f\"Top 5 similar words to '{word}': {top_similar_words(word, word_embeddings_df, top_n=5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea993e-949f-4f6a-b7d6-8cc0a3f01693",
   "metadata": {},
   "source": [
    "Compare Book Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792e21df832835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.908113Z",
     "start_time": "2024-11-17T19:44:32.036169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the book summary vectors\n",
    "doc_embedding = U\n",
    "\n",
    "# Compute cosine similarity between documents\n",
    "doc_similarities = cosine_similarity(doc_embedding)\n",
    "doc_similarities_df = pd.DataFrame(\n",
    "    doc_similarities, columns=[f\"Summary {i+1}\" for i in range(len(corpus))], index=[f\"Summary {i+1}\" for i in range(len(corpus))])\n",
    "\n",
    "#print(\"\\nSummary Similarities (Cosine Similarity):\\n\", doc_similarities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c0da9d1396235",
   "metadata": {},
   "source": [
    "**Question 2.** In this exercise, you will explore pre-trained word embeddings on the [Google News dataset](https://code.google.com/archive/p/word2vec/), and you will familiarize yourself with the **Gensim** package for handling the word embeddings. You will also investigate the analogy relationships between words based on their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b38cb2758cbac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.972729Z",
     "start_time": "2024-11-17T19:44:32.909025Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af1395fea97f1e",
   "metadata": {},
   "source": [
    "Download the pre-trained word embeddings from [Google News dataset](https://code.google.com/archive/p/word2vec/) and use the *Gensim* package to load and work with the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11a28ec7600eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:55:24.742520Z",
     "start_time": "2024-11-17T19:54:30.484276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path of the pretrained model\n",
    "googlenews_vectors = \n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format(googlenews_vectors, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e820c33d4aa987b",
   "metadata": {},
   "source": [
    " Use the following example list of words: 'king', 'queen', 'woman', 'man', 'fish', 'bird', 'snake', 'elephant' (or construct your own list of at least $6$-$10$ meaningful and diverse words.\n",
    "\n",
    "Reduce the dimensionality of the word embeddings to $2D$ space using PCA and visualize the words in the new latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17d9d6f8842524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:12:01.252221Z",
     "start_time": "2024-11-17T20:12:01.244256Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0, save=False, file_path='scatterplot.png'):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ecf0d99d9032d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:01:28.012839Z",
     "start_time": "2024-11-17T20:01:27.908570Z"
    }
   },
   "outputs": [],
   "source": [
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'fish', 'bird', 'snake', 'elephant',\n",
    "]\n",
    "display_pca_scatterplot(model=gensim_model, words=words_to_explore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46908d4ea44dcea3",
   "metadata": {},
   "source": [
    "Investigate the relationships between the words based on their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1332fcd75af78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:45:31.299852Z",
     "start_time": "2024-11-17T19:45:23.731390Z"
    }
   },
   "outputs": [],
   "source": [
    "gensim_model.most_similar(positive=[\"king\", \"woman\"],negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857e8aa81b4689b",
   "metadata": {},
   "source": [
    "Assess the overall accuracy of the embeddings and analyze strengths and weaknesses across analogy types (syntactic vs. semantic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa8e35e0103832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.783967Z",
     "start_time": "2024-11-17T19:45:31.302523Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = gensim_model.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "# The first entry stores the overall evaluation score on the entire evaluation set\n",
    "print(f\"Overall evaluation score: {accuracy[0]}\")\n",
    "for current_dict in accuracy[1]:\n",
    "    correct_count, incorrect_count = len(current_dict['correct']), len(current_dict['incorrect'])\n",
    "    section_accuracy = correct_count / float(correct_count + incorrect_count)\n",
    "    print(\n",
    "        f\"Section: {current_dict['section']} - Correct: {correct_count} - Incorrect: {incorrect_count} - Accuracy: {section_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1db6b95665a96b",
   "metadata": {},
   "source": [
    "**Question 3.** In this exercise, we will implement the **SkipGram** model of the **Word2Vec** framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bbfbd2f1a51a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.828352Z",
     "start_time": "2024-11-17T19:48:50.798432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77001a-d9f5-4000-884a-dac31f84bec4",
   "metadata": {},
   "source": [
    "Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c617105e7cf54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.854765Z",
     "start_time": "2024-11-17T19:48:50.833021Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus =  # Define the corpus (preprocessed summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a76a2c0543ae43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:00.934838Z",
     "start_time": "2024-11-17T20:02:00.073682Z"
    }
   },
   "outputs": [],
   "source": [
    "min_count = 5 # Discard the words that appear less or equal to 'min_count'\n",
    "window_size =  # You can choose small values to reduce computational cost, but it might affect the performance of the model\n",
    "\n",
    "# Define the vocabulary, i.e. set of all distinct words that appear more than 'min_count' times in the corpus.\n",
    "vocab = \n",
    "vocab_size = len(vocab) \n",
    "# Construct the dictionaries that maps words to unique ids and vice versa.\n",
    "word2Id = {word: id for id, word in enumerate(vocab)}\n",
    "id2Word = {id: word for word, id in word2Id.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd887f9bc9930c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:12.951331Z",
     "start_time": "2024-11-17T20:02:08.389981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate skip-gram training (source, context) word pairs.\n",
    "\n",
    "# You can implement the following function to generate the source-context word pairs.\n",
    "# If you wish, you can also modify the argument list\n",
    "def generate_center_context_pairs(sentences, window_size, word2Id, vocab):\n",
    "    '''\n",
    "    Given a list of sentences and a window size, generate a pair of context pairs\n",
    "    Note that a pair must be discarded if its center or context word appears less than 'min_count'.\n",
    "    :param sentences: a list of lists of words\n",
    "    :param window_size: window size\n",
    "    :param word2Id: a dictionary that maps words to ids\n",
    "    :param vocab: a set of unique words\n",
    "    :return: a list of centerId-contextId word pairs\n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "    return data\n",
    "\n",
    "pairs = generate_center_context_pairs(sentences=sentences, window_size=window_size, word2Id=word2Id, vocab=vocab)\n",
    "# Convert the list to a torch tensor\n",
    "pairs = torch.as_tensor(pairs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862e5643a37be1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:18.239299Z",
     "start_time": "2024-11-17T20:02:18.235440Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can examine the training data\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Number of center-context pairs: {pairs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a810b49-ff78-4fce-b463-dbe6086e48d4",
   "metadata": {},
   "source": [
    "Implement the SkipGram Model with Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c0cc84f4bc310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:28.635316Z",
     "start_time": "2024-11-17T20:02:28.630426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Skip-gram Model with Softmax\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Complete the implementation\n",
    "        \n",
    "    def forward(self, center_word):\n",
    "    \n",
    "    def get_center_embs(self, center_words = None):\n",
    "    \n",
    "    def get_context_embs(self, context_words):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378e530-e6c6-4ae3-a341-214188724c2e",
   "metadata": {},
   "source": [
    "Set the parameters required for training the model such as learning rate, embedding dimension, epochs and batch size.\n",
    "Note that chosen values for the parameters might/might not affect the optimization and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3fce033366089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:36.856677Z",
     "start_time": "2024-11-17T20:02:35.533543Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = \n",
    "epochs_num = \n",
    "embedding_dim = 100\n",
    "batch_size = \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Set the platform that we will use for training.\n",
    "device = torch.device('mps') #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be088d20-658c-4c2d-8c0d-428139547dc6",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d1923acf6a2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:09:04.234408Z",
     "start_time": "2024-11-17T20:02:45.290105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a data loader for pairs vector in pytorch\n",
    "data_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    total_loss = 0\n",
    "    for batch_pairs in tqdm(data_loader, desc=f\"Epoch {epoch} - Current batch progress\"):\n",
    "        # Transfer the batch data to the memory of the 'device' (i.e. gpu if used)\n",
    "        batch_pairs = batch_pairs.to(device)\n",
    "        # Define the center and context words\n",
    "        center_word, context_word = batch_pairs[:, 0], batch_pairs[:, 1]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center_word)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, context_word)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(pairs):.4f}\")  \n",
    "        \n",
    "# Transfer the model back to the cpu.\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf308d-7632-4555-abae-41925623a583",
   "metadata": {},
   "source": [
    "Generate Word Embeddings and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f35fa41e5e0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:13:52.994698Z",
     "start_time": "2024-11-17T20:13:52.901023Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'boy', 'daughter', \n",
    "    'dog', 'cat', 'horse', 'elephant', 'fish', 'bird', 'lizard', 'snake',\n",
    "    #'doctor', 'nurse', 'scientist', 'teacher', 'engineer', 'artist', 'musician', 'writer',\n",
    "    #'denmark', 'sweden', 'france', 'germany', 'spain', 'italy', 'portugal', 'turkey',\n",
    "]\n",
    "selected_ids = torch.tensor([word2Id[word] for word in words_to_explore])\n",
    "selected_word_embeddings = model.get_center_embs(selected_ids).detach()\n",
    "\n",
    "\n",
    "# Visualize the word embeddings using PCA for dimensionality reduction to 2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b986afc-127d-4bc6-8d2e-90c26062c9f5",
   "metadata": {},
   "source": [
    "Evaluate Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3908622e06954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:13:54.815663Z",
     "start_time": "2024-11-17T20:13:54.809638Z"
    }
   },
   "outputs": [],
   "source": [
    "names = [id2Word[i] for i in range(len(id2Word))]\n",
    "word_embs_df = pd.DataFrame(model.get_center_embs().detach(), index=names)\n",
    "\n",
    "word1, word2 = \"king\", \"queen\"\n",
    "# Test cosine similarity function\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity_terms(word1, word2, word_embs_df)}\")\n",
    "word = \"king\"\n",
    "print(f\"Top 5 similar words to '{word}': {top_similar_words(word, word_embs_df, names, top_n=5)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7afdf4c81d7aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
