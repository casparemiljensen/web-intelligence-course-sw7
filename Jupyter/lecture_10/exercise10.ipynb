{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5a9e28-680a-4347-9c2c-34b6bd6d6556",
   "metadata": {},
   "source": [
    "### Web Intelligence - Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af618479-764d-4275-86ae-41a6fcd0859b",
   "metadata": {},
   "source": [
    "In this exercise, we will explore several prominent techniques for learning word representations. First, we will implement a matrix decomposition-based word embedding model, **Latent Semantic Analysis (LSA)**. These models leverage matrix factorization to capture semantic relationships in text, providing an introduction to the underlying principles of word representation.\n",
    "\n",
    "Next, we will implement a neural-based approach, the **SkipGram** model of the **Word2Vec** framework. The SkipGram architecture is designed to predict surrounding context words given a target word, effectively learning word representations that capture meaningful relationships between words in a corpus.\n",
    "\n",
    "We will work on the [CMU Book Summary](https://www.cs.cmu.edu/~dbamman/booksummaries.html) dataset that consists of plot summaries for $16,559$ books extracted from Wikipedia. Throughout the exercise, we will analyze and compare these methods, discussing their advantages, limitations, and practical applications in natural language processing tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64444b45-6407-4a99-aaa4-48eb4fc77ab8",
   "metadata": {},
   "source": [
    "**Question 1.** In this exercise, we will implement the Latent Semantic Analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Software\\python\\Web Intelligence\\web-intelligence-course-sw7\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-11T12:28:07.366130900Z",
     "start_time": "2025-01-11T12:28:07.351850600Z"
    }
   },
   "id": "26d326306a46370a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff37314-ed5b-4bc0-9d65-9a7532fbf904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:33:32.456093600Z",
     "start_time": "2025-01-11T12:33:21.598649200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Software\\python\\Web\n",
      "[nltk_data]     Intelligence\\web-intelligence-course-\n",
      "[nltk_data]     sw7\\Jupyter\\lecture_10\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Software\\python\\Web\n",
      "[nltk_data]     Intelligence\\web-intelligence-course-\n",
      "[nltk_data]     sw7\\Jupyter\\lecture_10\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Software\\python\\Web\n",
      "[nltk_data]     Intelligence\\web-intelligence-course-\n",
      "[nltk_data]     sw7\\Jupyter\\lecture_10\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Software\\python\\Web\n",
      "[nltk_data]     Intelligence\\web-intelligence-course-\n",
      "[nltk_data]     sw7\\Jupyter\\lecture_10\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import nltk\n",
    "\n",
    "# Set custom NLTK data directory\n",
    "project_dir = os.path.dirname(os.path.abspath('exercise10.ipynb'))  # Get current script directory\n",
    "nltk_data_dir = os.path.join(project_dir, \"nltk_data\")    # Define 'nltk_data' directory in your project\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Add the directory to NLTK data path\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download resources to the custom path\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_dir)  # Optional: For multilingual support\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# For loading the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Preprocessing packages\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29251f-9d2d-4e95-89dd-b2601ef9a5b9",
   "metadata": {},
   "source": [
    "Load the dataset and preprocess the book summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e016111-703e-455f-9bb2-57a9cf5b4831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:33:41.039972400Z",
     "start_time": "2025-01-11T12:33:36.757806200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the CMU Book Summary Corpus dataset and get all the summaries.\n",
    "ld = load_dataset(\"textminr/cmu-book-summaries\")['train']\n",
    "summaries = [data['summary'] for data in ld]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d66ecc6aeec364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:34:08.825865200Z",
     "start_time": "2025-01-11T12:33:43.415275300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pre-processing text:   0%|          | 0/16559 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f68db72750c9491ca1a2eadd55b86426"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete the following preprocessing steps:\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_texts(texts, stop_words):\n",
    "    \n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in tqdm(texts, desc=\"Pre-processing text\"):\n",
    "        \n",
    "        # 1. Lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 3. Remove punctuation and special characters &&\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # 4. Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        # 5. Remove stopwords\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # 6. Lemmatize the tokens \n",
    "        words = [lemmatizer.lemmatize(word) for word in words]  # Use verb part-of-speech\n",
    "        \n",
    "        # Join tokens back into a single string\n",
    "        cleaned_texts.append(\" \".join(words))\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "# Run the preprocessing function on the summaries\n",
    "corpus = preprocess_texts(summaries, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1ce7808d87eda9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:02:37.843067800Z",
     "start_time": "2025-01-11T13:02:37.824069300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Old Major, the old boar on the Manor Farm, calls the animals on the farm for a meeting, where he co\n",
      "Preprocessed: old major old boar manor farm call animal farm meeting compare human parasite teach animal revolutio\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You can compare the original and preprocessed summaries to see the difference.\n",
    "print(f\"Original: {summaries[0][:100]}\")\n",
    "print(f\"Preprocessed: {corpus[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55aea1-60e1-4599-8b5d-329f9b056c52",
   "metadata": {},
   "source": [
    "Construct the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c0781140bd436c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:07:07.871113900Z",
     "start_time": "2025-01-11T13:07:05.429421100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Terms:\n",
      " ['10' '100' '1000' '10000' '100000' '11' '12' '12yearold' '13' '13yearold']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note: you can only consider the top 'N' words ordered by term frequency across the corpus.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "# Create a CountVectorizer for the DTM\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", max_features=vocab_size)\n",
    "\n",
    "# Fit and transform the corpus to create the DTM\n",
    "dt_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Extract the feature names (top N words)\n",
    "terms = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the first few terms\n",
    "print(\"\\nTerms:\\n\", terms[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7add3-344f-483d-b89c-7304d62fa377",
   "metadata": {},
   "source": [
    "Construct the Term Frequency-Inverse Document Frequency (TF-IDF) Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2e0cdbde08a54f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:16:04.825714700Z",
     "start_time": "2024-11-21T20:16:04.725837800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "   (0, 133)\t0.04150027829700491\n",
      "  (0, 163)\t0.01787838408697166\n",
      "  (0, 166)\t0.024047851437112504\n",
      "  (0, 169)\t0.021486533255623004\n",
      "  (0, 205)\t0.019968958385918494\n",
      "  (0, 210)\t0.02540244616021007\n",
      "  (0, 214)\t0.023890674032071246\n",
      "  (0, 248)\t0.023424717904722572\n",
      "  (0, 250)\t0.028157402173279493\n",
      "  (0, 352)\t0.09253021954306415\n",
      "  (0, 388)\t0.020200623927263895\n",
      "  (0, 397)\t0.027587918903752555\n",
      "  (0, 409)\t0.02486030023038575\n",
      "  (0, 482)\t0.5900330892276815\n",
      "  (0, 499)\t0.03844616591751648\n",
      "  (0, 548)\t0.013860535224677348\n",
      "  (0, 590)\t0.015843323840120098\n",
      "  (0, 598)\t0.019011204347495193\n",
      "  (0, 660)\t0.021661851751714506\n",
      "  (0, 695)\t0.021978249273469835\n",
      "  (0, 728)\t0.012483440929445712\n",
      "  (0, 733)\t0.03216625739596574\n",
      "  (0, 814)\t0.010648110333267907\n",
      "  (0, 924)\t0.01343425136825808\n",
      "  (0, 939)\t0.042125905444246735\n",
      "  :\t:\n",
      "  (4, 8816)\t0.03207509032705052\n",
      "  (4, 8819)\t0.04728338622713203\n",
      "  (4, 8839)\t0.046573697498296214\n",
      "  (4, 8849)\t0.0378884534874158\n",
      "  (4, 8860)\t0.03687043879351056\n",
      "  (4, 8980)\t0.04640558564393819\n",
      "  (4, 8982)\t0.10643145801888247\n",
      "  (4, 9077)\t0.024760398403784464\n",
      "  (4, 9094)\t0.06928839666445809\n",
      "  (4, 9097)\t0.06592584459547178\n",
      "  (4, 9209)\t0.03985173892849139\n",
      "  (4, 9264)\t0.024578688746411653\n",
      "  (4, 9437)\t0.05235785363812987\n",
      "  (4, 9450)\t0.03714847801716904\n",
      "  (4, 9491)\t0.042939708947135835\n",
      "  (4, 9560)\t0.04033491157235668\n",
      "  (4, 9590)\t0.04139743250005307\n",
      "  (4, 9709)\t0.04255153848081187\n",
      "  (4, 9715)\t0.022014102981270465\n",
      "  (4, 9757)\t0.018858370000569804\n",
      "  (4, 9764)\t0.03219885788470404\n",
      "  (4, 9908)\t0.07745293267691145\n",
      "  (4, 9963)\t0.0175089852752625\n",
      "  (4, 9974)\t0.01958241009603968\n",
      "  (4, 9998)\t0.28444775642469833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Initialize the transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit and transform the document-term matrix to the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(dt_matrix)\n",
    "\n",
    "# Display the first few terms\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117d862-2934-44b2-9ba8-11bdfaa7f20a",
   "metadata": {},
   "source": [
    "Perfom Singular Value Decomposition (SVD) and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45228affb9bf0d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:01.335336Z",
     "start_time": "2024-11-17T19:44:00.914167Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "k= 32\n",
    "\n",
    "U, S, Vh = linalg.svd(tfidf_matrix, k=k)\n",
    "\n",
    "# Print the singular values\n",
    "print(\"\\nTop k- singular values:\\n\", S)\n",
    "\n",
    "# Define the word embeddings\n",
    "# Note that the columns of SVh.T define the word embeddings, i.e. rows of the VhS\n",
    "# SVh.T is also equal to U^TX so the rows of (X^TU) also correspond the same word embeddings.\n",
    "word_embeddings = np.dot(np.diag(S), Vh).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d9622-4ff7-4016-8a9c-d0a7e6aade9e",
   "metadata": {},
   "source": [
    "Generate Word Embeddings and Analyze Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aac9536b614fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T21:40:32.804101500Z",
     "start_time": "2024-11-19T21:40:32.095316Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'man', 'woman', 'boy', 'daughter', \n",
    "    #'dog', 'cat', 'horse', 'elephant', 'fish', 'bird', 'lizard', 'snake',\n",
    "    #'doctor', 'nurse', 'scientist', 'teacher', 'engineer', 'artist', 'musician', 'writer',\n",
    "    #'denmark', 'sweden', 'france', 'germany', 'spain', 'italy', 'portugal', 'turkey',\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa279de7b9b5671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.033215Z",
     "start_time": "2024-11-17T19:44:32.021217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing similarity between words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_terms(word1, word2, word_embeddings_df):\n",
    "    \n",
    "    return \n",
    "\n",
    "# Find top 5 similar words for a given word\n",
    "def top_similar_words(word, word_embeddings_df, top_n=5):\n",
    "\n",
    "    return \n",
    "\n",
    "word_embeddings_df = pd.DataFrame(word_embeddings, index=terms)\n",
    "\n",
    "word1, word2 = \"king\", \"queen\"\n",
    "# Test cosine similarity function\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity_terms(word1, word2, word_embeddings_df)}\")\n",
    "word = \"king\"\n",
    "print(f\"Top 5 similar words to '{word}': {top_similar_words(word, word_embeddings_df, top_n=5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea993e-949f-4f6a-b7d6-8cc0a3f01693",
   "metadata": {},
   "source": [
    "Compare Book Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3792e21df832835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.908113Z",
     "start_time": "2024-11-17T19:44:32.036169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the book summary vectors\n",
    "doc_embedding = U\n",
    "\n",
    "# Compute cosine similarity between documents\n",
    "doc_similarities = cosine_similarity(doc_embedding)\n",
    "doc_similarities_df = pd.DataFrame(\n",
    "    doc_similarities, columns=[f\"Summary {i+1}\" for i in range(len(corpus))], index=[f\"Summary {i+1}\" for i in range(len(corpus))])\n",
    "\n",
    "#print(\"\\nSummary Similarities (Cosine Similarity):\\n\", doc_similarities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c0da9d1396235",
   "metadata": {},
   "source": [
    "**Question 2.** In this exercise, you will explore pre-trained word embeddings on the [Google News dataset](https://code.google.com/archive/p/word2vec/), and you will familiarize yourself with the **Gensim** package for handling the word embeddings. You will also investigate the analogy relationships between words based on their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b38cb2758cbac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:44:32.972729Z",
     "start_time": "2024-11-17T19:44:32.909025Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af1395fea97f1e",
   "metadata": {},
   "source": [
    "Download the pre-trained word embeddings from [Google News dataset](https://code.google.com/archive/p/word2vec/) and use the *Gensim* package to load and work with the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11a28ec7600eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:55:24.742520Z",
     "start_time": "2024-11-17T19:54:30.484276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the path of the pretrained model\n",
    "googlenews_vectors = \n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format(googlenews_vectors, binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e820c33d4aa987b",
   "metadata": {},
   "source": [
    " Use the following example list of words: 'king', 'queen', 'woman', 'man', 'fish', 'bird', 'snake', 'elephant' (or construct your own list of at least $6$-$10$ meaningful and diverse words.\n",
    "\n",
    "Reduce the dimensionality of the word embeddings to $2D$ space using PCA and visualize the words in the new latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17d9d6f8842524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:12:01.252221Z",
     "start_time": "2024-11-17T20:12:01.244256Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0, save=False, file_path='scatterplot.png'):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ecf0d99d9032d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:01:28.012839Z",
     "start_time": "2024-11-17T20:01:27.908570Z"
    }
   },
   "outputs": [],
   "source": [
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'fish', 'bird', 'snake', 'elephant',\n",
    "]\n",
    "display_pca_scatterplot(model=gensim_model, words=words_to_explore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46908d4ea44dcea3",
   "metadata": {},
   "source": [
    "Investigate the relationships between the words based on their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1332fcd75af78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:45:31.299852Z",
     "start_time": "2024-11-17T19:45:23.731390Z"
    }
   },
   "outputs": [],
   "source": [
    "gensim_model.most_similar(positive=[\"king\", \"woman\"],negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857e8aa81b4689b",
   "metadata": {},
   "source": [
    "Assess the overall accuracy of the embeddings and analyze strengths and weaknesses across analogy types (syntactic vs. semantic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa8e35e0103832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.783967Z",
     "start_time": "2024-11-17T19:45:31.302523Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = gensim_model.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "# The first entry stores the overall evaluation score on the entire evaluation set\n",
    "print(f\"Overall evaluation score: {accuracy[0]}\")\n",
    "for current_dict in accuracy[1]:\n",
    "    correct_count, incorrect_count = len(current_dict['correct']), len(current_dict['incorrect'])\n",
    "    section_accuracy = correct_count / float(correct_count + incorrect_count)\n",
    "    print(\n",
    "        f\"Section: {current_dict['section']} - Correct: {correct_count} - Incorrect: {incorrect_count} - Accuracy: {section_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1db6b95665a96b",
   "metadata": {},
   "source": [
    "**Question 3.** In this exercise, we will implement the **SkipGram** model of the **Word2Vec** framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bbfbd2f1a51a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.828352Z",
     "start_time": "2024-11-17T19:48:50.798432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77001a-d9f5-4000-884a-dac31f84bec4",
   "metadata": {},
   "source": [
    "Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c617105e7cf54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T19:48:50.854765Z",
     "start_time": "2024-11-17T19:48:50.833021Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus =  # Define the corpus (preprocessed summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a76a2c0543ae43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:00.934838Z",
     "start_time": "2024-11-17T20:02:00.073682Z"
    }
   },
   "outputs": [],
   "source": [
    "min_count = 5 # Discard the words that appear less or equal to 'min_count'\n",
    "window_size =  # You can choose small values to reduce computational cost, but it might affect the performance of the model\n",
    "\n",
    "# Define the vocabulary, i.e. set of all distinct words that appear more than 'min_count' times in the corpus.\n",
    "vocab = \n",
    "vocab_size = len(vocab) \n",
    "# Construct the dictionaries that maps words to unique ids and vice versa.\n",
    "word2Id = {word: id for id, word in enumerate(vocab)}\n",
    "id2Word = {id: word for word, id in word2Id.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd887f9bc9930c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:12.951331Z",
     "start_time": "2024-11-17T20:02:08.389981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate skip-gram training (source, context) word pairs.\n",
    "\n",
    "# You can implement the following function to generate the source-context word pairs.\n",
    "# If you wish, you can also modify the argument list\n",
    "def generate_center_context_pairs(sentences, window_size, word2Id, vocab):\n",
    "    '''\n",
    "    Given a list of sentences and a window size, generate a pair of context pairs\n",
    "    Note that a pair must be discarded if its center or context word appears less than 'min_count'.\n",
    "    :param sentences: a list of lists of words\n",
    "    :param window_size: window size\n",
    "    :param word2Id: a dictionary that maps words to ids\n",
    "    :param vocab: a set of unique words\n",
    "    :return: a list of centerId-contextId word pairs\n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "    return data\n",
    "\n",
    "pairs = generate_center_context_pairs(sentences=sentences, window_size=window_size, word2Id=word2Id, vocab=vocab)\n",
    "# Convert the list to a torch tensor\n",
    "pairs = torch.as_tensor(pairs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862e5643a37be1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:18.239299Z",
     "start_time": "2024-11-17T20:02:18.235440Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can examine the training data\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Number of center-context pairs: {pairs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a810b49-ff78-4fce-b463-dbe6086e48d4",
   "metadata": {},
   "source": [
    "Implement the SkipGram Model with Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c0cc84f4bc310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:28.635316Z",
     "start_time": "2024-11-17T20:02:28.630426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Skip-gram Model with Softmax\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Complete the implementation\n",
    "        \n",
    "    def forward(self, center_word):\n",
    "    \n",
    "    def get_center_embs(self, center_words = None):\n",
    "    \n",
    "    def get_context_embs(self, context_words):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378e530-e6c6-4ae3-a341-214188724c2e",
   "metadata": {},
   "source": [
    "Set the parameters required for training the model such as learning rate, embedding dimension, epochs and batch size.\n",
    "Note that chosen values for the parameters might/might not affect the optimization and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3fce033366089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:02:36.856677Z",
     "start_time": "2024-11-17T20:02:35.533543Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = \n",
    "epochs_num = \n",
    "embedding_dim = 100\n",
    "batch_size = \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Set the platform that we will use for training.\n",
    "device = torch.device('mps') #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") (for Mac M1/M2, torch.device('mps'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be088d20-658c-4c2d-8c0d-428139547dc6",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d1923acf6a2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:09:04.234408Z",
     "start_time": "2024-11-17T20:02:45.290105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a data loader for pairs vector in pytorch\n",
    "data_loader = DataLoader(pairs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    total_loss = 0\n",
    "    for batch_pairs in tqdm(data_loader, desc=f\"Epoch {epoch} - Current batch progress\"):\n",
    "        # Transfer the batch data to the memory of the 'device' (i.e. gpu if used)\n",
    "        batch_pairs = batch_pairs.to(device)\n",
    "        # Define the center and context words\n",
    "        center_word, context_word = batch_pairs[:, 0], batch_pairs[:, 1]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center_word)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, context_word)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(pairs):.4f}\")  \n",
    "        \n",
    "# Transfer the model back to the cpu.\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf308d-7632-4555-abae-41925623a583",
   "metadata": {},
   "source": [
    "Generate Word Embeddings and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f35fa41e5e0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:13:52.994698Z",
     "start_time": "2024-11-17T20:13:52.901023Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "words_to_explore= [\n",
    "    'king', 'queen', 'woman', 'man', 'boy', 'daughter', \n",
    "    'dog', 'cat', 'horse', 'elephant', 'fish', 'bird', 'lizard', 'snake',\n",
    "    #'doctor', 'nurse', 'scientist', 'teacher', 'engineer', 'artist', 'musician', 'writer',\n",
    "    #'denmark', 'sweden', 'france', 'germany', 'spain', 'italy', 'portugal', 'turkey',\n",
    "]\n",
    "selected_ids = torch.tensor([word2Id[word] for word in words_to_explore])\n",
    "selected_word_embeddings = model.get_center_embs(selected_ids).detach()\n",
    "\n",
    "\n",
    "# Visualize the word embeddings using PCA for dimensionality reduction to 2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b986afc-127d-4bc6-8d2e-90c26062c9f5",
   "metadata": {},
   "source": [
    "Evaluate Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3908622e06954",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T20:13:54.815663Z",
     "start_time": "2024-11-17T20:13:54.809638Z"
    }
   },
   "outputs": [],
   "source": [
    "names = [id2Word[i] for i in range(len(id2Word))]\n",
    "word_embs_df = pd.DataFrame(model.get_center_embs().detach(), index=names)\n",
    "\n",
    "word1, word2 = \"king\", \"queen\"\n",
    "# Test cosine similarity function\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {cosine_similarity_terms(word1, word2, word_embs_df)}\")\n",
    "word = \"king\"\n",
    "print(f\"Top 5 similar words to '{word}': {top_similar_words(word, word_embs_df, names, top_n=5)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7afdf4c81d7aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
